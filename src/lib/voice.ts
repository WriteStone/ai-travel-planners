// Voice recognition using Web Speech API
export class VoiceRecognition {
  private recognition: any
  private isSupported: boolean
  private isRecording: boolean = false

  constructor() {
    // Check if running in browser
    if (typeof window === 'undefined') {
      this.isSupported = false
      this.recognition = null
      return
    }

    // Check if browser supports Web Speech API
    this.isSupported = 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window
    
    if (this.isSupported) {
      const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition
      this.recognition = new SpeechRecognition()
      this.recognition.lang = 'zh-CN'
      this.recognition.continuous = true  // æ”¹ä¸ºæŒç»­è¯†åˆ«
      this.recognition.interimResults = true  // å¯ç”¨ä¸­é—´ç»“æžœ
      this.recognition.maxAlternatives = 1
      
      // æ·»åŠ ç»“æŸäº‹ä»¶ç›‘å¬
      this.recognition.onend = () => {
        this.isRecording = false
        console.log('è¯­éŸ³è¯†åˆ«ç»“æŸ')
      }
    }
  }

  isAvailable(): boolean {
    return this.isSupported
  }

  start(onResult: (text: string) => void, onError?: (error: any) => void): void {
    if (!this.isSupported) {
      onError?.(new Error('Speech recognition not supported'))
      return
    }

    // å¦‚æžœå·²ç»åœ¨å½•éŸ³,å…ˆåœæ­¢
    if (this.isRecording) {
      console.log('è¯­éŸ³è¯†åˆ«å·²åœ¨è¿è¡Œ,å…ˆåœæ­¢...')
      this.recognition.stop()
      // ç­‰å¾…ä¸€å°æ®µæ—¶é—´åŽå†å¯åŠ¨
      setTimeout(() => {
        this.startRecognition(onResult, onError)
      }, 300)
      return
    }

    this.startRecognition(onResult, onError)
  }

  private startRecognition(onResult: (text: string) => void, onError?: (error: any) => void): void {
    let finalTranscript = ''
    let interimTranscript = ''
    let silenceTimer: NodeJS.Timeout | null = null
    
    this.recognition.onresult = (event: any) => {
      interimTranscript = ''
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript
        if (event.results[i].isFinal) {
          finalTranscript += transcript
          console.log('è¯†åˆ«åˆ°å®Œæ•´å†…å®¹:', transcript)
        } else {
          interimTranscript += transcript
          console.log('è¯†åˆ«ä¸­:', transcript)
        }
      }
      
      // æ¸…é™¤ä¹‹å‰çš„é™é»˜è®¡æ—¶å™¨
      if (silenceTimer) {
        clearTimeout(silenceTimer)
      }
      
      // è®¾ç½®æ–°çš„é™é»˜è®¡æ—¶å™¨ï¼š2ç§’æ²¡æœ‰æ–°çš„è¯­éŸ³è¾“å…¥å°±åœæ­¢
      silenceTimer = setTimeout(() => {
        if (finalTranscript.trim()) {
          console.log('æœ€ç»ˆè¯†åˆ«ç»“æžœ:', finalTranscript)
          this.stop()
          onResult(finalTranscript.trim())
        }
      }, 2000)
    }

    this.recognition.onerror = (event: any) => {
      console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error)
      this.isRecording = false
      if (silenceTimer) {
        clearTimeout(silenceTimer)
      }
      
      if (event.error === 'aborted' || event.error === 'no-speech') {
        // æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³
        onError?.(new Error('æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•'))
        return
      }
      
      if (event.error === 'network') {
        onError?.(new Error('ç½‘ç»œé”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥'))
        return
      }
      
      onError?.(new Error('è¯­éŸ³è¯†åˆ«å¤±è´¥: ' + event.error))
    }

    this.recognition.onspeechend = () => {
      console.log('æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ')
      // è¯­éŸ³ç»“æŸåŽå†ç­‰1ç§’
      if (silenceTimer) {
        clearTimeout(silenceTimer)
      }
      silenceTimer = setTimeout(() => {
        if (finalTranscript.trim()) {
          this.stop()
          onResult(finalTranscript.trim())
        }
      }, 1000)
    }

    try {
      this.recognition.start()
      this.isRecording = true
      console.log('ðŸŽ¤ è¯­éŸ³è¯†åˆ«å·²å¯åŠ¨ - è¯·å¼€å§‹è¯´è¯')
    } catch (error) {
      console.error('å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥:', error)
      this.isRecording = false
      onError?.(error)
    }
  }

  stop(): void {
    if (this.recognition && this.isRecording) {
      try {
        this.recognition.stop()
        this.isRecording = false
        console.log('è¯­éŸ³è¯†åˆ«å·²åœæ­¢')
      } catch (error) {
        console.error('åœæ­¢è¯­éŸ³è¯†åˆ«å¤±è´¥:', error)
      }
    }
  }

  isCurrentlyRecording(): boolean {
    return this.isRecording
  }
}

// iFlytek Voice Recognition (for better Chinese support)
export interface IFlyTekConfig {
  appId: string
  apiKey: string
  apiSecret: string
}

export class IFlyTekVoiceRecognition {
  private config: IFlyTekConfig
  private websocket: WebSocket | null = null

  constructor(config: IFlyTekConfig) {
    this.config = config
  }

  async startRecording(onResult: (text: string) => void, onError?: (error: any) => void): Promise<void> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      const audioContext = new AudioContext()
      const source = audioContext.createMediaStreamSource(stream)
      const processor = audioContext.createScriptProcessor(4096, 1, 1)

      // Connect to iFlytek WebSocket API
      const url = await this.getWebSocketUrl()
      this.websocket = new WebSocket(url)

      this.websocket.onopen = () => {
        console.log('WebSocket connected')
      }

      this.websocket.onmessage = (event) => {
        const data = JSON.parse(event.data)
        if (data.data && data.data.result) {
          const text = data.data.result.ws.map((w: any) => w.cw.map((c: any) => c.w).join('')).join('')
          onResult(text)
        }
      }

      this.websocket.onerror = (error) => {
        onError?.(error)
      }

      processor.onaudioprocess = (e) => {
        if (this.websocket?.readyState === WebSocket.OPEN) {
          const inputData = e.inputBuffer.getChannelData(0)
          const pcmData = this.convertToPCM(inputData)
          this.websocket.send(JSON.stringify({
            data: {
              status: 1,
              format: 'audio/L16;rate=16000',
              audio: this.arrayBufferToBase64(pcmData),
              encoding: 'raw'
            }
          }))
        }
      }

      source.connect(processor)
      processor.connect(audioContext.destination)
    } catch (error) {
      onError?.(error)
    }
  }

  stopRecording(): void {
    if (this.websocket) {
      this.websocket.send(JSON.stringify({
        data: { status: 2 }
      }))
      this.websocket.close()
      this.websocket = null
    }
  }

  private async getWebSocketUrl(): Promise<string> {
    // This would need proper implementation with signature generation
    // For now, return a placeholder
    const host = 'iat-api.xfyun.cn'
    const path = '/v2/iat'
    return `wss://${host}${path}`
  }

  private convertToPCM(float32Array: Float32Array): ArrayBuffer {
    const int16Array = new Int16Array(float32Array.length)
    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]))
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7fff
    }
    return int16Array.buffer
  }

  private arrayBufferToBase64(buffer: ArrayBuffer): string {
    const bytes = new Uint8Array(buffer)
    let binary = ''
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i])
    }
    return btoa(binary)
  }
}

// Text-to-Speech for expense voice input
export function speakText(text: string): void {
  if ('speechSynthesis' in window) {
    const utterance = new SpeechSynthesisUtterance(text)
    utterance.lang = 'zh-CN'
    window.speechSynthesis.speak(utterance)
  }
}
